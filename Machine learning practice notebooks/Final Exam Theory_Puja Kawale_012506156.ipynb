{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam – Testing the Concepts\n",
    "## <font color=Green> Student Name: Puja Kawale </font><br>\n",
    "    <font color=Green>Date:12/11/2018                              \n",
    "    SJSU ID: 012506156\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=orange>Question 1) Describe the Scikit-learn Pipeline function with an example.</font><br>\n",
    "Answer 1) Definition: Sklearn pipeline is a set of chained algorithms. It extracts features, preprocess and Train them. With this it becomes easier to automate redundant steps in machine learning modeling process. \n",
    "\n",
    "•\tSteps in pipeline: Tuples\n",
    "•\tFirst element: Name of the step\n",
    "•\tSecond element: Transformer\n",
    "•\tThird element: Estimator (classifier/regressor/neural network/unsupervised algorithm)\n",
    "\n",
    "\n",
    "Sample Function:\n",
    "from sklearn. pipeline import Pipeline\n",
    "from sklearn. Tree import DecisionTreeClassifier\n",
    "from sklearn. feature_extraction.text import CountVectorizer\n",
    "\n",
    "pipeline = Pipeline (steps= [ ('vectorize', CountVectorizer ()), ('classify', DecisionTreeClassifier ())])\n",
    "\n",
    "raw features = ['Hello world', 'Machine learning is awesome']\n",
    "raw labels = [0, 1]\n",
    "\n",
    "pipeline. Fit (raw features, raw labels)\n",
    "\n",
    "Sample = ['Hi world']\n",
    "output = pipeline. Predict(sample)\n",
    "\n",
    "To train the estimator we should be calling the method ‘fit’ on the pipeline and provide the data to train. Advantage of using pipeline is we can put in raw data, which gets preprocessed automatically by the pipeline before running it through the estimator for training. After training the estimator it can be used to predict an output using the predict method. While predicting an output, the pipeline preprocesses the data before running it through the estimator to predict the result. The main take away is, we don't have to worry about differences in preprocessing between training and prediction. It is automatically done in the pipeline.\n",
    "\n",
    "\n",
    "<font color=orange>Question 2) What are the steps in NLP pre-processing?</font><br>\n",
    "Answer 2) NLP Preprocessing: \n",
    "•\tRemove Punctuation: Punctuation does not add value in NLP in most cases.\n",
    "•\tRemove stop words – Stop words are common words found in a language. Example:  for, of, are etc. are common stop words. They contribute less to the topic. English stop words and own customized stop words can be eliminated in this step.\n",
    "•\tLowercasing: It will avoid classifying the text based on case\n",
    "•\tRemove Number: Mostly Irrelevant in NLP\n",
    "•\tStrip white space – Avoiding the extra spaces (white spaces)\n",
    "•\tStemming – Transforms to root word. Stemming uses an algorithm that removes prefixes and suffixes in English words, such as “es”, “ed” and “’s”. For example, i.e., 1) “calculator” & “calculate” become “calculat”\n",
    "•\tParts of Speech Tagging This would allocate the speech tags to words, such as noun, verb, adjective, and so on. This step is necessary to perform prior to applying the lemmatization to bring down the word to its root word.\n",
    "•\tLemmatization – transform to dictionary base form i.e., “reduce” & “reduced” become “reduce”.\n",
    "•\tSparse terms –Removing infrequent terms for document term matrix.\n",
    "•\tTokenization- Tokenization describes splitting paragraphs into sentences, or sentences into individual words\n",
    "•\tWord Embedding/Text Vectors - Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors.\n",
    "\n",
    "<font color=orange>Question3) What is Deep Learning?</font><br>\n",
    "Answer 3) Deep learning is a subset of machine learning and machine learning is a subset of artificial intelligence. In deep learning tasks are broken down and distributed into machine learning algorithms that are organized into consecutive layers. Each layer is developed on the previous subsequent layer. Together these layers constitute neural network which is similar to the neurons in human brain. Deep learning algorithms are similar to how nervous system structured where each neuron connected each other and passing information. Deep learning models work in layers and a typical model atleast have three layers. Each layer accepts the information from previous and pass it on to the next one. Deep learning models tend to perform well with amount of data where as old machine learning models stops improving after a saturation point. \n",
    "\n",
    "Input (X) --- > Layer 1 (data transformation) _weight --- > Layer 1 (data transformation) _weight -- > Prediction (Y)\n",
    "<Feature extraction + Classification>\n",
    "One of differences between machine learning and deep learning model is on the feature extraction area. Feature extraction is done by human in machine learning whereas deep learning model figure out by itself.\n",
    "\n",
    "\n",
    "<font color=orange>Question 4) How does logistic regression compare to SVM? What if we have SVM with only binary classifications?</font><br>\n",
    "Answer 4) Differences between SVM and Logistic regression -\n",
    "•\tSVM try to maximize the margin between the closest support vectors while LR the posterior class probability. Thus, SVM find a solution which is as fare as possible for the two categories while LR does not.\n",
    "•\tLR is more sensitive to outliers than SVM because the cost function of LR diverges faster than those of SVM. \n",
    "•\tLogistic Regression produces probabilistic values while SVM produces 1 or 0. \n",
    "•\tSVM tries to find the widest possible separating margin, while Logistic Regression optimizes the log likelihood function, with probabilities modeled by the sigmoid function.\n",
    "•\tSVM extends by using kernel tricks, transforming datasets into rich features space, so that complex problems can be still dealt with in the same “linear” fashion in the lifted hyper space.\n",
    "\n",
    "SVMs are specialists in binary classification. But there are different strategies for utilizing them for multiclass issues. The foremost common strategies include changing the issue into a set of binary classification issues, by one of two strategies: One vs. the rest. For kk classes, kk parallel classifiers are prepared. Each decides whether a case has a place to its 'own' lesson versus any other course. The classifier with the biggest yield is taken to be the course of the example.\n",
    "\n",
    "\n",
    "<font color=orange>Question 5) Compare decision trees, random forests and SVM</font><br>\n",
    "Answer 5) Below is the differences: \n",
    "\n",
    "Decision tree Vs Random Forest: \n",
    "\n",
    "•\tDecision tress is one single tree that classifies the problem, whereas Random Forest is whole lot of individual trees that contribute in classifying the problem.\n",
    "•\tDecision tree uses entropy or information gain to decide to split the node into further sub-nodes. Random Forest works on Bagging concept of bootstrap aggregation. \n",
    "•\tIn Random Forests n trees are formed using sampling technique from n samples drawn from main dataset. Different combination od rows and columns are made use of. While in decision tree whole dataset considering all rows and columns.\n",
    "•\tDecision tree cannot handle large /huge data, Random forest can.\n",
    "•\tDecision tree tends to become overfitting random forest does not\n",
    "•\tRandom forest is Robust that decision tree.\n",
    "•\tDecision trees are computationally faster than Random Forest.\n",
    "•\tRandom forests are difficult to interpret, while decision tree is easily interpretable and can be converted to rules.\n",
    "\n",
    "Decision tree Vs SVM: \n",
    "\n",
    "•\tThe SVM (linear or other) uses a single decision hyperplane. The decision trees, are not bound to a single hyperplane: they use multiple decision rules. \n",
    "•\tSVM will generally perform better on linear dependencies, otherwise you need nonlinear kernel and choice of kernel may change results\n",
    "•\tSVM are less interpretable while Decision trees have better interpretability, \n",
    "•\tThey work faster and if you have categorical/numerical variables its fine, moreover: non-linear dependencies are handled well (given N large enough). \n",
    "•\tDecision trees get trained faster than SVM but they have tendency to overfit\n",
    "\n",
    "Random Forest Vs SVM: \n",
    "\n",
    "•\tRandom Forest is suited for multiclass problems, while SVM is binary in nature and expertise.\n",
    "•\tFor a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability.\n",
    "•\tRandom Forest you can use data as they are. SVM maximizes the \"margin\" and thus relies on the concept of \"distance\" between different points.\n",
    "•\tRandom forests actually don't take long to train, especially if you do so in parallel, something one cannot do with SVM or boosted trees. They are fairly robust and have very little need for tuning of hyperparameters. \n",
    "•\tThere are a lot of entities to be turned in SVMs: Choosing the “right” kernel, regularization penalties, the slack variable, while in Random Forest it’s not the case.\n",
    "\n",
    "<font color=orange>Question 6) What is Distillation?</font><br>\n",
    "Answer 6) Distillation is condensing and filtering of the raw unstructured data.\n",
    "Steps in Distillation: \n",
    "•\tData Preprocessing: Include all NLP Preprocessing Steps.\n",
    "•\tSentiment analysis: Sentiment Analysis is a sub-field of Natural Language Processing (NLP) that tries to identify and extract opinions within a given text. The aim of sentiment analysis is to identify the attitude, sentiments, evaluations, attitudes and emotions of a speaker/writer based on the computational power\n",
    "VADER (Valence Aware Dictionary and sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "•\tTopic modelling: Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Techniques: LDA, LSA, pLSA.\n",
    "•\tRanking: Can be achieved using Cosine Similarity of documents and then sorting them to suggest the Urgency, Severity, Importance.\n",
    "\n",
    "\n",
    "<font color=orange> Question7) What is an Activation Function? </font> <br>\n",
    "Answer 7) Activation functions are features of the artificial neural networks. They decide whether a neuron should be activated or not. Also, if the information that the neuron is receiving is important should be answered or ignored.\n",
    "Formula - Y = Activation (summation(weight*input) +Bias\n",
    "\n",
    "Binary Step Function: This function makes use of a threshold. A binary step function with a threshold T is given by,\n",
    "\n",
    "f(x) = 1 if x >= T f(x) = 0 if x < T\n",
    "\n",
    "Linear Function: The gradient being zero in step function, it was impossible to update gradient during the backpropagation. Hence, we can try using a linear function\n",
    "Sigmoid Function: The sigmoid functions are extensively used in back propagation neural networks because it reduces the burden of complication involved during training phase.\n",
    "Tanh Function: Scaled version of the sigmoid function.\n",
    "\n",
    "Note: Sigmoid functions and their combinations generally work better in the case of classifiers. Sigmoid and tanh functions are sometimes avoided due to the vanishing gradient problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
