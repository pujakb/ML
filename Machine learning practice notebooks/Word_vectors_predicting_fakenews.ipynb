{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read the training data <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('train.tsv','r',encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d =lines[0].strip().split('\\t')\n",
    "for i in range(0,len(d)):\n",
    "    print (i,d[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Label 1 for true & mostly true , Label 0 for fake or partially true statements </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = []\n",
    "false_data = []\n",
    "overall_data = []\n",
    "label = []\n",
    "for i in range(0,len(lines)):\n",
    "    line = lines[i].strip().lower().split()\n",
    "    overall_data.append(lines[i].strip().lower())\n",
    "    if (line[1]== 'true' or line[1] =='mostly-true'):\n",
    "        true_data.append(lines[i].strip().lower())\n",
    "        label.append(1)\n",
    "    else:\n",
    "        false_data.append(lines[i].strip().lower())\n",
    "        label.append(0)\n",
    "## Please note overall data still contains all of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please note overall data still contains all of training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>use nltk word tokenize and remove stop words </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-19b1ecf08591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##use nltk word tokenize and remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mparty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "##use nltk word tokenize and remove stop words\n",
    "data = []\n",
    "for line in vocabulary:\n",
    "    text = vocabulary['words']\n",
    "    party = line.split('\\t')[7].lower()\n",
    "    line = party + ' ' + text \n",
    "    tokens = [i for i in word_tokenize(line.strip().lower()) if i not in stop]\n",
    "    bigrams = []\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigram = tokens[i]+'-'+tokens[i+1]\n",
    "        bigrams.append(bigram)\n",
    "    for bigram in bigrams:\n",
    "        tokens.append(bigram)\n",
    "        \n",
    "    data.append(tokens)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Use Gensim and create word vectors from the dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code referenced from https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "print(model['democrat'])\n",
    "# save model\n",
    "model.save('overall_model.bin')\n",
    "# load model\n",
    "overall_model = Word2Vec.load('overall_model.bin')\n",
    "print(overall_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use word vectors as features \n",
    "## sentence made of multiple words, then we simply add the vectors of each word and get single vector for that sentence and use that as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for line in data:\n",
    "    vectors = []\n",
    "    for word in line:\n",
    "        vector = overall_model[word]\n",
    "        vectors.append(vector)\n",
    "    word_vector = []\n",
    "\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        word_vector.append(val)\n",
    "    X.append(word_vector)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(label)\n",
    "scaled_X = X_df\n",
    "scaled_Y = Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, scaled_Y, test_size=0.2, random_state=0)\n",
    "lm = linear_model.LogisticRegression(verbose=1)\n",
    "model = lm.fit(X_train, y_train)\n",
    "print (model)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "print (\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression was trained on word vectors and it has accuracy of 0.64 in predicting the fake news on test set. let us test on sample sentence \n",
    "lets say if republican says \"obama supports third trimester abortion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing= \"republican obama supports third trimester abortion\"\n",
    "\n",
    "import numpy as np\n",
    "def getVector(s):\n",
    "    s = s.lower()\n",
    "    words = word_tokenize(s)\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in overall_model:\n",
    "            vectors.append(overall_model[word])\n",
    "    sum_vector = []\n",
    "    for i in range(0,len(vectors[0])):\n",
    "        val = 0.0\n",
    "        for j in range(0,len(vectors)):\n",
    "            val += vectors[j][i]\n",
    "        sum_vector.append(val)\n",
    "    return pd.DataFrame([sum_vector])\n",
    "\n",
    "testX = getVector(testing)\n",
    "result_score = model.predict(testX)\n",
    "model.predict_proba(testX)\n",
    "### 0.71 probability that news belong to class 2 which is fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 probability is 0.28 and class 2 probability is 0.71 , so its likely a fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
