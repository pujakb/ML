{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pdf file\n",
    "files = ['Document1.pdf','Document2.pdf','Document3.pdf','Document4.pdf','Document5.pdf','Document6.pdf']\n",
    "\n",
    "for filename in files:\n",
    "    #open allows you to read the file\n",
    "    pdfFileObj = open(filename,'rb')\n",
    "    #The pdfReader variable is a readable object that will be parsed\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    #discerning the number of pages will allow us to parse through all #the pages\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    #The while loop will read each page\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "    \n",
    "    #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "    if text != \"\":\n",
    "       text = text\n",
    "    #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "    else:\n",
    "       text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "    # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "    # Now, we will clean our text variable, and return it as a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into keywords\n",
    "\n",
    "#The word_tokenize() function will break our text phrases into #individual words\n",
    "tokens = word_tokenize(text)\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = filter(unicode.isalnum, keywords)\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#import sys  \n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf-8')\n",
    "\n",
    "spam_list = [\"Free\", \"Click here\", \"visit\", \"open attachment\", \"call this number\", \"money\", \"Out\", \"extra\", \"offer\", \n",
    " \"available\", \"Pension\", \"Opportunity\", \"Chance\", \"Investment\", \"Pension\"]\n",
    "\n",
    "#with open('spam-output.csv', 'wb') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    for val in keywords:\n",
    "#        if val in spam_list:\n",
    "#            value = val + \",\" + \"spam\"\n",
    "#            writer.writerow([value])\n",
    "#       else:\n",
    "#           value = val + \",\" + \"notspam\"\n",
    "#           writer.writerow([value])\n",
    "\n",
    "# Since most of the spam_or_not columns contains not spam, Manually adding spam keywords to the csv to train\n",
    "keyword_spam_list = [\"Free\", \"Click here\", \"visit\", \"open attachment\", \"call this number\", \"money\", \"Out\", \"extra\", \"offer\", \n",
    " \"Chance\", \"Investment\", \"Pension\"]\n",
    "\n",
    "for each_value in keyword_spam_list:\n",
    "    keywords.append(each_value)\n",
    "\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "from itertools import izip_longest as zip_longest\n",
    "spam_or_not = []\n",
    "d = [keywords, spam_or_not]\n",
    "export_data = zip_longest(*d, fillvalue = '')\n",
    "for val in keywords:\n",
    "    if val in spam_list:\n",
    "        # 0 for spam\n",
    "        spam_or_not.append(\"0\")\n",
    "    else:\n",
    "        # 1 for not spam\n",
    "        spam_or_not.append(\"1\")\n",
    "with open('spam-output.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"keywords\", \"spam_or_not\"))\n",
    "    wr.writerows(export_data)\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#import pandas as pd\n",
    "#dataset = pd.read_csv(\"spam-output.csv\")\n",
    "#corpus = []\n",
    "#for i in range(len(keywords)):\n",
    "#    keywords = re.sub('[^a-zA-Z]', '', dataset['keywords'][i])\n",
    "#    corpus.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating bag of words model\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"spam-output.csv\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 150)\n",
    "X = dataset.iloc[:, 0].values\n",
    "X = cv.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 1].values\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying naive bayes classifier\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
